Requisitos e exigencias do tcc

ORIENTAÇÕES
Prezados alunos,
Nesse documento estão apresentadas as informações necessárias para o desenvolvimento do seu
Trabalho de Conclusão de Curso. O TCC nos cursos de especialização da PUC Minas Virtual é um
trabalho interdisciplinar. Nosso propósito é consolidar os conhecimentos aprendidos no curso, dando
ao aluno mais uma oportunidade de colocá-los em prática em um contexto de trabalho.
Nesse TCC você terá duas opções. Você poderá escolher entre duas opções: (1) escrever um trabalho
teórico (com a entrega de um artigo de no máximo 20 páginas) ou (2) desenvolver um projeto de
Engenharia de Dados utilizando os conceitos, técnicas e ferramentas estudadas durante o curso.
O contexto e problema a ser abordado no TCC deve ser escolhido pelo aluno. Dessa forma, espera-se
que os conhecimentos possam ser aplicados em um projeto alinhado com os interesses do aluno,
mas que seja um problema REAL e RELEVANTE para sua organização ou para a sociedade. Nesse
ponto, é importante ressaltar que, mesmo tendo a possibilidade de escolha do tema, o aluno deverá
observar cuidadosamente e cumprir o conjunto de requisitos e as restrições técnicas que fazem parte
deste trabalho, descritos no item “Escopo do Trabalho”.
Os itens que devem ser produzidos para o fechamento da disciplina são descritos no item “O que deve ser entregue”? que fica ao final deste documento.
O trabalho deve ser feito individualmente.
Em nosso modelo de especialização, não há a orientação acadêmica/metodológica através de um
professor orientador. Com base no formato escolhido, artigo ou projeto, você irá elaborar o texto do
seu trabalho. Em caso de dúvidas sobre as regras ou procedimentos do TCC, o aluno deve enviar uma
mensagem no Fórum de Discussões da disciplina. Desta forma, espera-se que este espaço sirva de
orientação para todos os alunos, uma vez que todos terão acesso às perguntas realizadas pelos
colegas e os esclarecimentos sobre elas.


OPÇÃO 1: ARTIGO
Caso você faça a opção por escrever um trabalho teórico, você deverá submeter seu artigo no
formato da Revista Abakós, do Instituto de Ciências Exatas e Informática. Os templates para WORD
ou LATEX estão disponíveis aqui. Seu artigo deve ter no máximo 20 páginas e tem tema aberto. O
trabalho teórico deve ter qualidade científica e apresentar uma visão diferenciada sobre algum tema,
lançando novas luzes a respeito. Isso vai exigir grande profundidade e poder de questionamento.
Você pode falar sobre novas tecnologias, aplicações, soluções de problemas em Engenharia de
Dados; também pode fazer um estudo comparativo entre métodos, técnicas ou padrões, de caráter
teórico e/ou prático, adotando como referência um benchmark. Deve ser fortemente apoiado em
referências.

OPÇÃO 2: PROJETO PRÁTICO
A seguir estão as etapas que você deve seguir caso tenha optado por fazer um projeto prático. Nesse
caso você deverá entregar um relatório técnico detalhando todas as etapas do projeto.

1. Definição do Problema
No seu TCC, primeiramente você deverá escolher um PROBLEMA REAL de seu interesse em qualquer
área: transporte, economia, consumo, educação, saúde, etc. Você tem liberdade para escolher o
tema que achar mais interessante, mas é importante que esse seja um problema RELEVANTE para
sua organização ou para a sociedade. É muito importante que nessa primeira etapa você defina
muito bem a pergunta que o seu trabalho vai resolver usando as técnicas e ferramentas da
Engenharia de Dados.
 
2. Modelagem conceitual e definição das Tecnologias/Ferramentas/Arquitetura
A partir da clara definição do seu problema, você deve agora buscar as fontes de dados para o seu
projeto, fazer a modelagem de dados, identificar as tecnologias/ferramentas que serão utilizadas em
cada etapa (preparação e ingestão, orquestração, processamento, e visualização).
Como sugestão para obtenção de datasets, você pode buscar em sites como o Kaggle, o famoso
repositório de dados da Universidade da Califórnia em Irvine (UCI), os dados abertos do governo
federal (dados.gov.br) ou de governos de outros países (EUA – data.gov, Reino Unido – data.gov.uk).
O artigo “Best Public Datasets for Machine Learning and Data Science” dá excelentes dicas de
repositórios de dados. Sugerimos o uso de diferentes formatos para que você possa colocar em
prática tudo o que aprendeu: formato estruturado, não-estruturado e semi-estruturado).
Observação: caso você queira utilizar dados da empresa onde você trabalha, sugerimos que você
remova ou camufle qualquer dado que identifique pessoas ou organizações.

3. Ingestão de Dados
Após a obtenção dos seus dados, é momento de elaborar rotinas para a ingestão de dados
(preparação, ETL, workflows, carga e armazenamento, streaming). A sua solução deve apresentar um
mecanismo automatizado (processos ETL) para o processamento das fontes de dados utilizadas de tal
forma que, caso os dados sejam atualizados e a estrutura seja mantida, não seja necessário o
desenvolvimento de procedimento adicional para visualizar as alterações dos dados.

4. Orquestração de Dados
Nessa etapa você vai elaborar rotinas para a orquestração de dados. Deixe bem claro em seu
documento quais foram as estratégias utilizadas. 

5. Visualização dos dados
Nessa última etapa você deve elaborar rotinas para a visualização dos dados através de painéis de
indicadores, dashboards, etc.




ENTREGA DO TRABALHO
Para realizar a entrega do trabalho, você deve postar o seu TCC conforme o template disponibilizado,
em formato PDF. Os links para o vídeo no Youtube e para o repositório contendo dados e demais
arquivos (scripts, etc) devem estar contidos nesse documento.
Após avaliação das entregas postadas no Canvas, os professores da banca irão indicar os alunos aptos
para a apresentação final do TCC.

O QUE DEVE SER ENTREGUE?
● Artigo ou relatório técnico conforme template disponibilizado (em formato PDF). Lembre-se
de detalhar o máximo possível. Informe as ferramentas utilizadas, onde e quando coletou os
dados, etc…
● Link para vídeo no Youtube. Esse vídeo terá tempo máximo de 5 minutos e deverá
apresentar de forma sucinta o seu projeto, desde a definição do problema, a obtenção e
tratamento dos dados, até a apresentação dos resultados.
APRESENTAÇÃO DO TRABALHO PARA A BANCA
● Você deve preparar um conjunto de slides no PowerPoint (ou algum software equivalente)
para apresentar seu trabalho para a banca de professores. Geralmente o tempo de
apresentação é de 15 a 20 minutos.
DÚVIDAS?
Nosso objetivo foi disponibilizar todos os materiais necessários para a execução do trabalho.
Entretanto, entendemos que dúvidas podem surgir. Neste caso, mande uma mensagem para a gente
no fórum de discussão. Bom trabalho!




-----------------------------------------------------------------------------
anotações

1. Análise Inicial dos Dados
Dê uma olhada nos primeiros registros de cada DataFrame (df.head()) para entender a estrutura e o formato dos dados.
Verifique os tipos de dados, identifique colunas relevantes e avalie se há necessidade de conversões.
2. Limpeza de Dados
Trate valores ausentes, duplicatas e quaisquer inconsistências nos dados.
Verifique se os dados estão em um formato que facilita a análise.
3. Transformação de Dados
Se necessário, faça transformações nos dados para atender aos requisitos do seu projeto.
Crie novas colunas, aplique funções de agregação, etc.
4. União dos DataFrames
Considere se faz sentido unir os DataFrames (por exemplo, concatenar ou mesclar) ou se você manterá análises separadas.
5. Documentação
Adicione comentários ao seu código para explicar as etapas importantes.
Se necessário, crie um notebook Markdown para documentar as decisões de transformação e limpeza.
Se você tiver alguma dúvida específica ou quiser orientações sobre uma etapa específica, sinta-se à vontade para compartilhar!


Verificação de Dados:

Certifique-se de que os dados foram carregados corretamente, verificando as primeiras linhas do DataFrame (head()).
Confira se os tipos de dados de cada coluna estão corretos.
Tratamento de Dados Ausentes:

Verifique se há dados ausentes (NaN) e decida como tratá-los. Você pode preenchê-los com valores padrão, remover linhas ou colunas, ou aplicar técnicas mais avançadas de imputação.
Tratamento de Dados Duplicados:

Verifique se há duplicatas nos dados e remova-as, se necessário, para evitar distorções nos resultados da análise.
Padronização de Nomes de Colunas:

Certifique-se de que os nomes das colunas estão em um formato consistente e amigável.
Conversão de Tipos de Dados:

Se necessário, converta os tipos de dados das colunas para otimizar o uso de memória e garantir a consistência.
Exploração Inicial:

Realize uma exploração inicial dos dados para entender suas características e identificar possíveis padrões ou peculiaridades.



http://dados.recife.pe.gov.br/dataset/area-urbana/resource/698d8fe1-d30a-485f-8d5d-307d46163d0c?inner_span=True

https://pt.wikipedia.org/wiki/Predefini%C3%A7%C3%A3o:Subdivis%C3%B5es_de_Recife

http://dados.recife.pe.gov.br/dataset/area-urbana/resource/8740c5db-2670-474f-895a-70a0628a7ed1

diario de tcc
11/01/24 
nao usarei mais a base de 2018 pq as informações a respeito de condições da via estao muito sujas. com muitos nulos em comparação as bases dos anos posteriores

a base de 2019 nao tem dados duplicados

campos que serão usados de 2019 (definir chave primaria)
DATA
hora
natureza_acidente - com vitima/semvitima
situacao	
bairro
bairro_cruzamento (comparar com bairro pra entender se é a mesma coisa)
tipo - tipo colisao
auto	
moto	
ciclom	
ciclista	
pedestre	
onibus	
caminhao	
viatura	
outros	
vitimas - entender se o numero que retorna aqui e o numero de pessoas
acidente_verificado	
tempo_clima
sinalizacao	
condicao_via	
conservacao_via
mao_direcao 

CAMPOS DE 2020

DATA
hora
natureza_acidente
situacao	
bairro
tipo
auto	
moto	
ciclom	
ciclista	
pedestre	
onibus	
caminhao	
viatura	
outros	
vitimas 
acidente_verificado	
tempo_clima
sinalizacao	
condicao_via	
conservacao_via
mao_direcao 



ver o padrao dos ultimos anos nos campos nulos e replicar

no de/para de bairros acima, usar a chave primaria de barrios (ver se ta tudo maiusculo ou menusculo)

https://chat.openai.com/share/adfc4654-6f3d-44e0-ae79-c9b54c264bb5

diaria dia 13
ajustar metadado de bairros do recife
pequena vitoria do dia = meu primeiro merge

pendencias de hoje
dropar a coluna de rgiao
mergiar o csv de rpa-ragiao na staging
fazer metadata de bairro 
entender como ficará a modelagem dos dados

LIMPAR NULOS NO RPA DA STAGIN


23/01
mudar a sequencia das colunas se for necessario
mudar o nome dos campos de natureza-acidente e acidente-verificado
checar campo de 2021 ok
terminar 2022

1- deixar todas as bases identicas (mesmo tratamento) e cruza apenas uma vez
2.concat()
3. ver se os bairros realmente nao estao nas bases
'ESTÂNCIA' 'BOMBA DO HEMETÉRIO' 'GRAÇAS' 'SÃO JOSÉ' 'SANTO ANTÔNIO'
 'TORREÃO' 'POÇO DA PANELA' 'TOTÓ' 'JARDIM SÃO PAULO' 'VÁRZEA' 'HIPÓDROMO'
 'BAIRRO DO RECIFE' 'CAXANGÁ' 'OUTROS' 'ALTO JOSÉ BONIFÁCIO' 'JORDÃO'
 'ÁGUA FRIA' 'TEJIPIÓ' 'ALTO JOSÉ DO PINHO' 'JIQUIÁ' 'DOIS IRMÃOS'
 'TORRÕES' 'MORRO DA CONCEIÇÃO' 'JOANA BEZERRA' 'CÓRREGO DO JENIPAPO'
 'CIDADE UNIVERSITÁRIA' 'CAÇOTE' 'SÍTIO DOS PINTOS' 'FUNDÃO'
 'ALTO SANTA TERESINHA' 'BRASÍLIA TEIMOSA'


3- fazer tudo virar csv e carregar em dados-carregados-tratados


fazer tudo local depois partir para aws
aws- salvar no lambda a automação
s3 - salvar a base de dados

Agendamento Automático: Utilize ferramentas de orquestração, como Apache Airflow ou cron jobs, para agendar automaticamente os processos ETL em intervalos regulares.

Monitoramento e Alertas: Implemente monitoramento contínuo para identificar falhas ou atrasos nos processos ETL. Configure alertas para notificar a equipe em caso de problemas.

Fluxo de Dependências: Defina dependências entre diferentes etapas do processo ETL para garantir que elas sejam executadas na ordem correta.

Paralelização: Se viável, considere a paralelização de tarefas para otimizar o desempenho e reduzir o tempo total de execução.

Log de Execução: Mantenha registros detalhados de cada execução do processo ETL, incluindo informações sobre sucesso, falhas e desempenho.

Escalonamento Dinâmico: Implemente uma estratégia que permita o escalonamento dinâmico de recursos computacionais conforme necessário, especialmente se o volume de dados variar ao longo do tempo.


s3-recife-accidents-data
layer-acidentes-recife

sudo amazon linux extras install python 3.8


sudo amazon-linux-extras install python3.8

python3.8 get-pip.py --user


mkdir python



python3.8 -m pip install pandas -t python/



zip -r layer.zip python


aws lambda publish-layer-version --layer-name pandas-layer --zip-file fileb://layer.zip --compatible-runtimes python3.8 --region us-east-1





31/01
todo que for read-csv voce vai ter que ler de um s3 (como fazer lambda ler um csv no s3)


linha 153 salvar o retorno no s3 (data-processed)

eventbridger (orquestração)
cron execução mensal



















